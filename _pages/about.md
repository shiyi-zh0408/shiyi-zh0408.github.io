---
permalink: /
title: "ðŸ‘‹ About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm currently a 1st-year PhD student at Tsinghua University Shenzhen International Graduate School, supervised by Prof. [Yansong Tang](https://andytang15.github.io/) and Prof. [Jiwen Lu](http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/biography.html). I got my bachelor's degree from the Department of Automation, Tsinghua University in 2023.

My research interests lie in Computer Vision, such as Video Understanding, Video Generation, and Embodied Visual Perception.

<p align="center">[Email](mailto:sy-zhang23@mails.tsinghua.edu.cn) / [Github](https://github.com/shiyi-zh0408)</p>

---
# âœ¨ News
---
* <span style="font-size: smaller;">2024-03: One paper on video understanding (Narrative Action Evaluation) is accepted to [CVPR 2024](https://cvpr.thecvf.com/)</span>
* <span style="font-size: smaller;">2023-03: One paper on video understanding (Action Quality Assessment) is accepted to [CVPR 2023](https://cvpr.thecvf.com/Conferences/2023)</span>

---
# ðŸ”¬ Research
---
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>	
  <!--ManiGaussian-->
  <tr>
    <td style="padding:20px;width:30%;max-width:30%" align="center">
      <img style="width:100%;max-width:100%" src="../images/maga.png" alt="dise">
    </td>
    <td width="75%" valign="center">
      <papertitle>ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation</papertitle>
      <br>
      Guanxing Lu, <b>Shiyi Zhang</b>, Ziwei Wang, Changliu Liu, Jiwen Lu and Yansong Tang.
      <br>
      <em>Preprint</em>
      <br>
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_LOGO_A_Long-Form_Video_Dataset_for_Group_Action_Quality_Assessment_CVPR_2023_paper.pdf">[PDF]</a>
      <a href="https://github.com/shiyi-zh0408/LOGO">[Project Page]</a> 
      <br>
      <p> We propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction.</p>
    </td>
  </tr>	

  <!--NAE-->
  <tr>
    <td style="padding:20px;width:30%;max-width:30%" align="center">
      <img style="width:100%;max-width:100%" src="../images/nae.png" alt="dise">
    </td>
    <td width="75%" valign="center">
      <papertitle>Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</papertitle>
      <br>
      <b>Shiyi Zhang*</b>, Sule Bai*, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, Yansong Tang
      <br>
      <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
      <br>
      <a href="">[PDF]</a>
      <a href="https://github.com/shiyi-zh0408/NAE_CVPR2024">[Project Page]</a> 
      <br>
      <p> We investigate a new problem called narrative action evaluation (NAE) and propose a prompt-guided multimodal interaction framework.</p>
    </td>
  </tr>	

  <!--LOGO-->
  <tr>
    <td style="padding:20px;width:30%;max-width:30%" align="center">
      <img style="width:100%;max-width:100%" src="../images/logo.png" alt="dise">
    </td>
    <td width="75%" valign="center">
      <papertitle>LOGO: A Long-Form Video Dataset for Group Action Quality Assessment</papertitle>
      <br>
      <b>Shiyi Zhang</b>, Wenxun Dai, Sujia Wang, Xiangwei Shen, Jiwen Lu, Jie Zhou, Yansong Tang
      <br>
      <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
      <br>
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_LOGO_A_Long-Form_Video_Dataset_for_Group_Action_Quality_Assessment_CVPR_2023_paper.pdf">[PDF]</a>
      <a href="https://github.com/shiyi-zh0408/LOGO">[Project Page]</a> 
      <br>
      <p> LOGO is a new multi-person long-form video dataset for action quality assessment.</p>
    </td>
  </tr>	
</tbody></table>

